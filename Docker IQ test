Reference: https://www.youtube.com/watch?v=I6ZBUEc4LrU&list=PLdpzxOOAlwvIKMhk8WhzN1pYoJ1YU8Csa&index=35

*********************************************************************************************************************
1. What is Docker?

Docker is a platform that enables developers to create, deploy, and run applications in containers. Containers are lightweight, portable, 
and self-sufficient units that include everything needed to run a piece of software, such as the code, runtime, libraries, and system tools. 
Here are some key aspects of Docker:

Key Features:
Containerization: Docker packages applications and their dependencies into containers, ensuring they run consistently across different environments.
Isolation: Containers provide process and filesystem isolation, allowing applications to run independently of the host system.
Portability: Docker containers can run on any system that supports Docker, making it easy to move applications between development, testing, and production environments.
Efficiency: Containers are lightweight and use fewer resources compared to traditional virtual machines, enabling faster startup times and better resource utilization.

Components:
Docker Engine: The core component that runs and manages containers.
Docker Hub: A repository for storing and sharing container images.
Docker Compose: A tool for defining and running multi-container Docker applications.

Example Use Case:
Imagine you have a web application that needs to run on multiple environments (development, testing, production). 
By packaging the application into a Docker container, you can ensure it runs consistently across all these environments without worrying 
about differences in system configurations.

Docker has revolutionized the way applications are developed, deployed, and managed, making it a cornerstone of modern DevOps practices.

*********************************************************************************************************************
2. What is a docker container and how is differnet from virtual machine ?

A Docker container is a lightweight, portable, and self-sufficient unit that includes everything needed to run a piece of software, 
such as the code, runtime, libraries, and system tools. Containers are designed to run consistently across different environments, 
ensuring that applications behave the same way regardless of where they are deployed.

Docker Container:
Isolation: Containers provide process and filesystem isolation, allowing multiple containers to run on the same host without interfering with each other.
Efficiency: Containers share the host operating system's kernel, making them more lightweight and faster to start compared to virtual machines.
Portability: Containers can run on any system that supports Docker, making it easy to move applications between development, testing, and production environments.

Virtual Machine (VM):
Isolation: VMs provide full isolation by virtualizing the entire hardware stack, including the operating system,
which allows multiple VMs to run on the same physical hardware.
Resource Usage: VMs are heavier because they include a full operating system along with the application and its dependencies. 
This results in higher resource usage and slower startup times.
Portability: VMs are less portable compared to containers because they are tied to the hypervisor and the underlying hardware.

Key Differences:
Architecture: Containers share the host OS kernel, while VMs include a full OS. This makes containers more lightweight and efficient.
Startup Time: Containers start much faster than VMs because they don't need to boot an entire operating system.
Resource Efficiency: Containers use fewer resources compared to VMs, allowing for higher density of applications on the same hardware.

Visual Comparison:
!Docker Container vs Virtual Machine

Example Use Case:
Containers: Ideal for microservices architectures, where each service runs in its own container, allowing for easy scaling and management.
VMs: Suitable for running multiple, isolated operating systems on a single physical machine, often used for legacy applications or when full OS isolation is required.

*********************************************************************************************************************
3. What is a docker file ?
A Dockerfile is a text file that contains a set of instructions for building a Docker image. 
It defines the environment in which your application will run, specifying everything from the base image to the commands needed 
to install dependencies and configure the application. Here’s a breakdown of its key components:

Key Components of a Dockerfile:
FROM: Specifies the base image to use for the Docker image.
FROM ubuntu:20.04

RUN: Executes commands to install dependencies or perform other setup tasks.
RUN apt-get update && apt-get install -y python3

COPY: Copies files from the host system into the Docker image.
COPY . /app

WORKDIR: Sets the working directory inside the Docker image.
WORKDIR /app

CMD: Specifies the command to run when the container starts.
CMD ["python3", "app.py"]

EXPOSE: Defines the port on which the container will listen.
EXPOSE 8080


Example Dockerfile:
Here’s a simple example of a Dockerfile for a Python application:
# Use an official Python runtime as a parent image
FROM python:3.8-slim

# Set the working directory in the container
WORKDIR /app

# Copy the current directory contents into the container at /app
COPY . /app

# Install any needed packages specified in requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

# Make port 80 available to the world outside this container
EXPOSE 80

# Run app.py when the container launches
CMD ["python", "app.py"]


Building and Running a Docker Image:
Build the Docker Image:
docker build -t my-python-app .


Run the Docker Container:
docker run -p 4000:80 my-python-app

Dockerfiles are essential for automating the creation of Docker images, ensuring that your application runs consistently across different environments.

*********************************************************************************************************************
4. What are docker images ?

Docker images are the blueprints for Docker containers. They contain everything needed to run an application, including the code, runtime, libraries, 
environment variables, and configuration files. Here’s a detailed look at Docker images:

Key Features of Docker Images
Immutable: Docker images are immutable, meaning once they are created, they cannot be changed. This ensures consistency across different environments.
Layered Structure: Docker images are built in layers. Each instruction in a Dockerfile creates a new layer, and these layers are stacked to form the final image. 
This layered approach allows for efficient storage and reuse of common layers.
Portable: Docker images can be easily shared and distributed, allowing applications to run consistently across different systems.

Creating Docker Images:
Docker images are typically created using a Dockerfile, which contains a set of instructions for building the image. Here’s a simple example:

# Use an official Python runtime as a parent image
FROM python:3.8-slim

# Set the working directory in the container
WORKDIR /app

# Copy the current directory contents into the container at /app
COPY . /app

# Install any needed packages specified in requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

# Make port 80 available to the world outside this container
EXPOSE 80

# Run app.py when the container launches
CMD ["python", "app.py"]



Building and Running Docker Images
Build the Docker Image:
# docker build -t my-python-app .


Run the Docker Container:
# docker run -p 4000:80 my-python-app

Using Docker Images:
Docker images can be stored in repositories like Docker Hub, making it easy to share and distribute them. 
You can pull images from these repositories and run them on any system that supports Docker.

Example Use Case:
Imagine you have a web application that needs to run on multiple environments (development, testing, production). 
By packaging the application into a Docker image, you can ensure it runs consistently across all these environments without worrying 
about differences in system configurations.

Docker images are fundamental to the containerization process, enabling the portability and consistency of applications across different environments.


*********************************************************************************************************************
5. What are the different docker components ?

Docker consists of several key components that work together to enable containerization and management of applications. Here are the main components:

Docker Engine:
Docker Daemon: The background service responsible for building, running, and managing Docker containers.
Docker CLI: The command-line interface used to interact with the Docker Daemon and manage Docker containers, images, networks, and volumes.

Docker Images:
Base Images: The foundational images from which other images are built. Examples include official images like ubuntu, alpine, and python.
Custom Images: Images created by users, typically defined using a Dockerfile.

Docker Containers:
Running Instances: Containers are the running instances of Docker images. They encapsulate the application and its dependencies, providing isolated environments.

Dockerfile:
Build Instructions: A text file containing a set of instructions for building a Docker image. It specifies the base image, dependencies, configurations, and commands to run.

Docker Compose:
Multi-Container Applications: A tool for defining and running multi-container Docker applications using a docker-compose.yml file. It simplifies the management of complex applications with multiple services.

Docker Hub:
Image Repository: A cloud-based registry where Docker images are stored and shared. Users can pull images from Docker Hub to run on their local systems.

Docker Swarm:
Container Orchestration: A native clustering and orchestration tool for Docker. It allows users to manage a cluster of Docker nodes and deploy services across them.

Docker Network:
Networking: Docker provides networking capabilities to connect containers to each other and to external networks. It includes bridge networks, host networks, and overlay networks.

Docker Volume:
Storage: Volumes are used to persist data generated by and used by Docker containers. They provide a way to share data between containers and retain data even after containers are stopped.

These components work together to provide a comprehensive platform for containerization, enabling developers to build, ship, and run applications efficiently.

*********************************************************************************************************************
6. What is a docker registry and which registry you are using in your current organisation ?
A Docker registry is a centralized location for storing and distributing Docker images. 
It allows users to push, pull, and manage container images. 
Docker registries can be public or private, and they help in sharing images across different environments and teams.

Key Features of Docker Registry:
Storage: Stores Docker images, which are collections of layers representing the filesystem and configuration of a container.
Distribution: Facilitates the distribution of images to different environments, ensuring consistency across development, testing, and production.
Versioning: Supports versioning of images through tags, allowing users to manage different versions of their applications 1.

Popular Docker Registries:
Docker Hub: The default public registry provided by Docker, widely used for sharing and distributing images 1.
Amazon Elastic Container Registry (ECR): A fully managed Docker registry provided by AWS 1.
Azure Container Registry (ACR): A managed Docker registry service provided by Microsoft Azure1.
Google Container Registry (GCR): A Docker registry service provided by Google Cloud1.
Harbor: An open-source registry that provides security features and management capabilities 1.

Example Usage:
To push an image to Docker Hub:
# docker tag my-image myusername/my-image:latest
# docker push myusername/my-image:latest

Registry in Your Organization
Bitbucket/GitLab Container Registry for enhanced security and control over their images.

*********************************************************************************************************************
7. What is the difference between docker COPY and docker ADD ?

Both COPY and ADD are instructions used in a Dockerfile to copy files and directories into a Docker image, but they have some differences:

COPY:
Basic Copying: COPY is used to copy files and directories from the local filesystem into the Docker image.
Syntax:
COPY <src> <dest>

Usage:
COPY . /app


Functionality: It performs a straightforward copy operation without any additional features.

ADD:
Enhanced Copying: ADD can do everything COPY does, but it also has additional features.
Syntax:
ADD <src> <dest>

Usage:
ADD . /app


Additional Features:
URL Handling: ADD can fetch files from URLs and add them to the image.
ADD https://example.com/file.tar.gz /app/

Archive Extraction: ADD can automatically extract local tar files into the specified directory.
ADD file.tar.gz /app/

Key Differences:
Functionality: COPY is simpler and should be used when you only need to copy files and directories. ADD is more powerful and can handle URLs and tar file extraction.
Best Practice: It is generally recommended to use COPY unless you specifically need the additional features provided by ADD, as COPY is more predictable and less prone to unexpected behavior.

*********************************************************************************************************************
8. What is the difference between CMD and Entrypoint in docker ?

Both CMD and ENTRYPOINT are instructions used in a Dockerfile to specify the command that should be run when a container starts. 
However, they serve slightly different purposes and have distinct behaviors:

CMD
Purpose: Specifies the default command to run when a container starts. It can be overridden by providing a command at runtime.
Syntax:
# CMD ["executable", "param1", "param2"]

or
CMD command param1 param2

Example:
CMD ["python", "app.py"]

Behavior: If a command is provided at runtime, it overrides the CMD instruction.
# docker run my-image python other-script.py

ENTRYPOINT
Purpose: Specifies the command that will always be executed when the container starts. It is not easily overridden.
Syntax:
# ENTRYPOINT ["executable", "param1", "param2"]

or
#vENTRYPOINT command param1 param2

Example:
# ENTRYPOINT ["python", "app.py"]

Behavior: The ENTRYPOINT instruction is not overridden by runtime commands. 
Instead, any additional arguments provided at runtime are passed as arguments to the ENTRYPOINT command.
# docker run my-image other-script.py

This would result in python app.py other-script.py.

Combining CMD and ENTRYPOINT:
Purpose: You can use CMD to provide default arguments to the ENTRYPOINT command.

Example:
ENTRYPOINT ["python"]
CMD ["app.py"]

This setup allows you to run the container with the default command (python app.py) or override the CMD arguments at runtime.
docker run my-image other-script.py

This would result in python other-script.py.

Key Differences:
Overriding: CMD can be easily overridden by runtime commands, while ENTRYPOINT cannot be overridden as easily.
Use Case: Use CMD for default commands that can be overridden, and ENTRYPOINT for commands that should always run.

*********************************************************************************************************************
9. How can you transfer a file from one docker container to another ?

Transferring files between Docker containers can be done using several methods. Here are a few common approaches:

1. Using docker cp Command
You can copy files between containers and your local filesystem using the docker cp command. 
First, copy the file from the source container to your local machine, then copy it from your local machine to the target container.

# Copy file from source container to local machine
docker cp <source_container>:/path/to/file /local/path

# Copy file from local machine to target container
docker cp /local/path/file <target_container>:/path/to/destination


2. Using Shared Volumes
You can create a shared volume that both containers can access. This way, any file written to the shared volume by one container can be read by the other container.

# Create a shared volume
docker volume create shared_volume

# Run containers with the shared volume
docker run -v shared_volume:/shared/path --name container1 <image>
docker run -v shared_volume:/shared/path --name container2 <image>

# Now, files written to /shared/path in container1 will be accessible in container2


3. Using Network Transfer
You can use network transfer tools like scp or rsync if your containers have network access to each other.
# Example using scp
docker exec <source_container> scp /path/to/file user@<target_container_ip>:/path/to/destination


4. Using docker exec Command
You can use docker exec to run commands inside containers to transfer files.
# Copy file from source container to target container using tar
docker exec <source_container> tar cf - /path/to/file | docker exec -i <target_container> tar xf - -C /path/to/destination


These methods should help you transfer files between Docker containers effectively


*********************************************************************************************************************
10. What is the difference between docker, docker compose and docker swarm ?

Docker:
Docker is a platform that allows you to develop, ship, and run applications inside containers. 
Containers are lightweight, portable, and self-sufficient units that include everything needed to run a piece of software, including the code, 
runtime, libraries, and system tools.

Docker Compose:
Docker Compose is a tool for defining and running multi-container Docker applications. 
With Docker Compose, you use a YAML file to configure your application's services. Then, with a single command, 
you can create and start all the services from your configuration.

Use Case: Ideal for defining and running multi-container applications on a single host.
Configuration: Uses a docker-compose.yml file to define services, networks, and volumes.

Example docker-compose.yml file:
version: '3'
services:
  web:
    image: nginx
    ports:
      - "80:80"
  db:
    image: mysql
    environment:
      MYSQL_ROOT_PASSWORD: example


Docker Swarm:
Docker Swarm is Docker's native clustering and orchestration tool. 
It allows you to turn a group of Docker engines into a single, virtual Docker engine. Swarm manages the scheduling 
and deployment of containers across multiple Docker hosts, ensuring high availability and scalability.

Use Case: Ideal for deploying and managing a cluster of Docker nodes in a production environment.
Configuration: Uses docker service commands and a docker-compose.yml file (with some differences) for defining services.

Key features of Docker Swarm:

Scalability: Easily scale up or down the number of container instances.
Load Balancing: Automatically distributes traffic across containers.
High Availability: Ensures that services are always running, even if some nodes fail.

Summary:
Docker: Core platform for containerization.
Docker Compose: Tool for defining and running multi-container applications on a single host.
Docker Swarm: Clustering and orchestration tool for managing a cluster of Docker nodes.

*********************************************************************************************************************
11. What is multistage build in docker ?

A multistage build in Docker is a technique that allows you to use multiple FROM statements in your Dockerfile to create intermediate images. 
This helps in optimizing the final image by reducing its size and including only the necessary components. Here's how it works:

Why Use Multistage Builds?
Smaller Image Size: By copying only the necessary artifacts from intermediate stages, you can significantly reduce the size of the final image.
Improved Security: Reduces the attack surface by excluding unnecessary tools and dependencies.
Better Organization: Helps in separating build dependencies from runtime dependencies.
How It Works
In a multistage build, you define multiple stages in your Dockerfile. Each stage starts with a FROM statement. 
You can then selectively copy artifacts from one stage to another using the COPY --from=<stage> instruction.

Example Dockerfile
Here's an example of a multistage build Dockerfile:

# Stage 1: Build Stage
FROM golang:1.16 AS builder
WORKDIR /app
COPY . .
RUN go build -o myapp

# Stage 2: Final Stage
FROM alpine:latest
WORKDIR /app
COPY --from=builder /app/myapp .
CMD ["./myapp"]



Explanation
Stage 1 (Builder):

Uses the golang:1.16 image to build the application.
Copies the source code into the container and builds the application.
Stage 2 (Final):

Uses a lightweight alpine:latest image.
Copies the built application from the builder stage.
Sets the command to run the application.
Benefits
Efficiency: Only the final stage's image is used in production, which is smaller and more efficient.
Flexibility: You can use different base images for different stages, optimizing each stage for its specific purpose.
Multistage builds are particularly useful for complex applications where the build environment differs significantly from the runtime environment.


*********************************************************************************************************************
12. What are distroless image in docker ?

Distroless images are a type of Docker image that contain only your application and its runtime dependencies, without including 
a full operating system or package managers 1. This approach focuses on minimizing the image size and reducing the attack surface by excluding unnecessary components.

Key Features of Distroless Images:
Minimal Size: Distroless images are significantly smaller than traditional images. For example, the smallest distroless image is around 2 MiB, 
compared to Alpine (~5 MiB) and Debian (~124 MiB) 1.
Security: By including only the necessary runtime dependencies, distroless images reduce the number of potential vulnerabilities and improve the 
signal-to-noise ratio for security scanners 2.
No Shell or Package Manager: Distroless images do not contain a shell, package manager, or other utilities typically found in standard Linux distributions 1.

Example Usage:
Distroless images are often used in production environments where security and efficiency are paramount. Here's an example of how you might use a 
distroless image in a Dockerfile:

# Stage 1: Build Stage
FROM golang:1.16 AS builder
WORKDIR /app
COPY . .
RUN go build -o myapp

# Stage 2: Final Stage
FROM gcr.io/distroless/base-debian12
WORKDIR /app
COPY --from=builder /app/myapp .
CMD ["./myapp"]


Benefits
Smaller Attack Surface: Fewer components mean fewer potential vulnerabilities.
Optimized Performance: Smaller images lead to faster deployment and reduced resource consumption.
Simplified Maintenance: Less software to update and manage.
Distroless images are particularly useful for applications where security and efficiency are critical.



*********************************************************************************************************************

